{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "(94, 246)\n",
      "(94, 5)\n",
      "time points: 13\n",
      "samples:  2042\n",
      "WARNING:tensorflow:From C:\\Users\\onoda\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-2-046c28cc6186>:182: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 246)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer1 (Dense)           (None, 32)           7904        inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           hidden_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer2 (Dense)           (None, 32)           1056        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32)           0           hidden_layer2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer3 (Dense)           (None, 32)           1056        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           hidden_layer3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "param1_layer (Dense)            (None, 1)            33          dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "param2_layer (Dense)            (None, 1)            33          dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "params_layer (Concatenate)      (None, 2)            0           param1_layer[0][0]               \n",
      "                                                                 param2_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask_bartch (InputLayer)        [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 13)           0           params_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,082\n",
      "Trainable params: 10,082\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "save initial weights\n",
      "save initial weights\n",
      "save initial weights\n",
      "save initial weights\n",
      "save initial weights\n",
      "Train on 1021 samples, validate on 1021 samples\n",
      "WARNING:tensorflow:From C:\\Users\\onoda\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/100\n",
      "1021/1021 - 1s - loss: 1.3992 - val_loss: 1.4199\n",
      "Epoch 2/100\n",
      "1021/1021 - 0s - loss: 1.2559 - val_loss: 1.0670\n",
      "Epoch 3/100\n",
      "1021/1021 - 0s - loss: 1.1035 - val_loss: 0.8439\n",
      "Epoch 4/100\n",
      "1021/1021 - 0s - loss: 0.9579 - val_loss: 0.7241\n",
      "Epoch 5/100\n",
      "1021/1021 - 0s - loss: 0.9036 - val_loss: 0.6602\n",
      "Epoch 6/100\n",
      "1021/1021 - 0s - loss: 0.8203 - val_loss: 0.6276\n",
      "Epoch 7/100\n",
      "1021/1021 - 0s - loss: 0.7835 - val_loss: 0.6062\n",
      "Epoch 8/100\n",
      "1021/1021 - 0s - loss: 0.8019 - val_loss: 0.5951\n",
      "Epoch 9/100\n",
      "1021/1021 - 0s - loss: 0.7345 - val_loss: 0.5917\n",
      "Epoch 10/100\n",
      "1021/1021 - 0s - loss: 0.6984 - val_loss: 0.5871\n",
      "Epoch 11/100\n",
      "1021/1021 - 0s - loss: 0.7144 - val_loss: 0.5807\n",
      "Epoch 12/100\n",
      "1021/1021 - 0s - loss: 0.6560 - val_loss: 0.5705\n",
      "Epoch 13/100\n",
      "1021/1021 - 0s - loss: 0.6420 - val_loss: 0.5537\n",
      "Epoch 14/100\n",
      "1021/1021 - 0s - loss: 0.6365 - val_loss: 0.5425\n",
      "Epoch 15/100\n",
      "1021/1021 - 0s - loss: 0.6031 - val_loss: 0.5341\n",
      "Epoch 16/100\n",
      "1021/1021 - 0s - loss: 0.5793 - val_loss: 0.5261\n",
      "Epoch 17/100\n",
      "1021/1021 - 0s - loss: 0.5789 - val_loss: 0.5196\n",
      "Epoch 18/100\n",
      "1021/1021 - 0s - loss: 0.5600 - val_loss: 0.5168\n",
      "Epoch 19/100\n",
      "1021/1021 - 0s - loss: 0.5152 - val_loss: 0.5082\n",
      "Epoch 20/100\n",
      "1021/1021 - 0s - loss: 0.5628 - val_loss: 0.5010\n",
      "Epoch 21/100\n",
      "1021/1021 - 0s - loss: 0.5240 - val_loss: 0.4936\n",
      "Epoch 22/100\n",
      "1021/1021 - 0s - loss: 0.5218 - val_loss: 0.4871\n",
      "Epoch 23/100\n",
      "1021/1021 - 0s - loss: 0.4954 - val_loss: 0.4826\n",
      "Epoch 24/100\n",
      "1021/1021 - 0s - loss: 0.5387 - val_loss: 0.4795\n",
      "Epoch 25/100\n",
      "1021/1021 - 0s - loss: 0.4791 - val_loss: 0.4708\n",
      "Epoch 26/100\n",
      "1021/1021 - 0s - loss: 0.4856 - val_loss: 0.4661\n",
      "Epoch 27/100\n",
      "1021/1021 - 0s - loss: 0.4869 - val_loss: 0.4627\n",
      "Epoch 28/100\n",
      "1021/1021 - 0s - loss: 0.4438 - val_loss: 0.4559\n",
      "Epoch 29/100\n",
      "1021/1021 - 0s - loss: 0.4557 - val_loss: 0.4464\n",
      "Epoch 30/100\n",
      "1021/1021 - 0s - loss: 0.4412 - val_loss: 0.4454\n",
      "Epoch 31/100\n",
      "1021/1021 - 0s - loss: 0.4490 - val_loss: 0.4434\n",
      "Epoch 32/100\n",
      "1021/1021 - 0s - loss: 0.4455 - val_loss: 0.4407\n",
      "Epoch 33/100\n",
      "1021/1021 - 0s - loss: 0.4049 - val_loss: 0.4391\n",
      "Epoch 34/100\n",
      "1021/1021 - 0s - loss: 0.4006 - val_loss: 0.4374\n",
      "Epoch 35/100\n",
      "1021/1021 - 0s - loss: 0.3744 - val_loss: 0.4339\n",
      "Epoch 36/100\n",
      "1021/1021 - 0s - loss: 0.4040 - val_loss: 0.4313\n",
      "Epoch 37/100\n",
      "1021/1021 - 0s - loss: 0.4139 - val_loss: 0.4313\n",
      "Epoch 38/100\n",
      "1021/1021 - 0s - loss: 0.4041 - val_loss: 0.4281\n",
      "Epoch 39/100\n",
      "1021/1021 - 0s - loss: 0.3871 - val_loss: 0.4258\n",
      "Epoch 40/100\n",
      "1021/1021 - 0s - loss: 0.3713 - val_loss: 0.4197\n",
      "Epoch 41/100\n",
      "1021/1021 - 0s - loss: 0.3669 - val_loss: 0.4204\n",
      "Epoch 42/100\n",
      "1021/1021 - 0s - loss: 0.3729 - val_loss: 0.4194\n",
      "Epoch 43/100\n",
      "1021/1021 - 0s - loss: 0.3750 - val_loss: 0.4208\n",
      "Epoch 44/100\n",
      "1021/1021 - 0s - loss: 0.3458 - val_loss: 0.4227\n",
      "Epoch 45/100\n",
      "1021/1021 - 0s - loss: 0.3339 - val_loss: 0.4221\n",
      "Epoch 46/100\n",
      "1021/1021 - 0s - loss: 0.3569 - val_loss: 0.4222\n",
      "Epoch 47/100\n",
      "1021/1021 - 0s - loss: 0.3596 - val_loss: 0.4208\n",
      "Epoch 48/100\n",
      "1021/1021 - 0s - loss: 0.3265 - val_loss: 0.4238\n",
      "Epoch 49/100\n",
      "1021/1021 - 0s - loss: 0.3466 - val_loss: 0.4266\n",
      "Epoch 50/100\n",
      "1021/1021 - 0s - loss: 0.3242 - val_loss: 0.4224\n",
      "Epoch 51/100\n",
      "1021/1021 - 0s - loss: 0.3162 - val_loss: 0.4213\n",
      "Epoch 52/100\n",
      "1021/1021 - 0s - loss: 0.2978 - val_loss: 0.4227\n",
      "Epoch 00052: early stopping\n",
      "1021/1021 [==============================] - 0s 79us/sample\n",
      "[[0.         0.        ]\n",
      " [0.80569106 0.        ]\n",
      " [0.80243902 0.        ]\n",
      " [0.80162602 0.        ]\n",
      " [0.80243902 0.        ]\n",
      " [0.80243902 0.        ]\n",
      " [0.80162602 0.        ]\n",
      " [0.80406504 0.        ]\n",
      " [0.80487805 0.        ]\n",
      " [0.80487805 0.        ]\n",
      " [0.80406504 0.        ]\n",
      " [0.80487805 0.        ]\n",
      " [0.80569106 0.        ]]\n",
      "load inital weights\n",
      "Train on 1021 samples, validate on 1021 samples\n",
      "Epoch 1/100\n",
      "1021/1021 - 1s - loss: 1.4411 - val_loss: 1.3708\n",
      "Epoch 2/100\n",
      "1021/1021 - 0s - loss: 1.2781 - val_loss: 1.0365\n",
      "Epoch 3/100\n",
      "1021/1021 - 0s - loss: 1.1183 - val_loss: 0.7988\n",
      "Epoch 4/100\n",
      "1021/1021 - 0s - loss: 1.0150 - val_loss: 0.6733\n",
      "Epoch 5/100\n",
      "1021/1021 - 0s - loss: 0.8693 - val_loss: 0.6068\n",
      "Epoch 6/100\n",
      "1021/1021 - 0s - loss: 0.8690 - val_loss: 0.5914\n",
      "Epoch 7/100\n",
      "1021/1021 - 0s - loss: 0.7840 - val_loss: 0.5698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "1021/1021 - 0s - loss: 0.7449 - val_loss: 0.5468\n",
      "Epoch 9/100\n",
      "1021/1021 - 0s - loss: 0.7608 - val_loss: 0.5314\n",
      "Epoch 10/100\n",
      "1021/1021 - 0s - loss: 0.7304 - val_loss: 0.5311\n",
      "Epoch 11/100\n",
      "1021/1021 - 0s - loss: 0.6767 - val_loss: 0.5183\n",
      "Epoch 12/100\n",
      "1021/1021 - 0s - loss: 0.6666 - val_loss: 0.5049\n",
      "Epoch 13/100\n",
      "1021/1021 - 0s - loss: 0.6877 - val_loss: 0.4955\n",
      "Epoch 14/100\n",
      "1021/1021 - 0s - loss: 0.6303 - val_loss: 0.4872\n",
      "Epoch 15/100\n",
      "1021/1021 - 0s - loss: 0.5942 - val_loss: 0.4812\n",
      "Epoch 16/100\n",
      "1021/1021 - 0s - loss: 0.5776 - val_loss: 0.4774\n",
      "Epoch 17/100\n",
      "1021/1021 - 0s - loss: 0.6268 - val_loss: 0.4788\n",
      "Epoch 18/100\n",
      "1021/1021 - 0s - loss: 0.5848 - val_loss: 0.4709\n",
      "Epoch 19/100\n",
      "1021/1021 - 0s - loss: 0.5807 - val_loss: 0.4659\n",
      "Epoch 20/100\n",
      "1021/1021 - 0s - loss: 0.5536 - val_loss: 0.4603\n",
      "Epoch 21/100\n",
      "1021/1021 - 0s - loss: 0.5410 - val_loss: 0.4604\n",
      "Epoch 22/100\n",
      "1021/1021 - 0s - loss: 0.5291 - val_loss: 0.4551\n",
      "Epoch 23/100\n",
      "1021/1021 - 0s - loss: 0.5312 - val_loss: 0.4473\n",
      "Epoch 24/100\n",
      "1021/1021 - 0s - loss: 0.4711 - val_loss: 0.4433\n",
      "Epoch 25/100\n",
      "1021/1021 - 0s - loss: 0.4958 - val_loss: 0.4406\n",
      "Epoch 26/100\n",
      "1021/1021 - 0s - loss: 0.4961 - val_loss: 0.4367\n",
      "Epoch 27/100\n",
      "1021/1021 - 0s - loss: 0.4802 - val_loss: 0.4329\n",
      "Epoch 28/100\n",
      "1021/1021 - 0s - loss: 0.4490 - val_loss: 0.4335\n",
      "Epoch 29/100\n",
      "1021/1021 - 0s - loss: 0.4666 - val_loss: 0.4319\n",
      "Epoch 30/100\n",
      "1021/1021 - 0s - loss: 0.4470 - val_loss: 0.4240\n",
      "Epoch 31/100\n",
      "1021/1021 - 0s - loss: 0.4149 - val_loss: 0.4196\n",
      "Epoch 32/100\n",
      "1021/1021 - 0s - loss: 0.4450 - val_loss: 0.4207\n",
      "Epoch 33/100\n",
      "1021/1021 - 0s - loss: 0.4555 - val_loss: 0.4228\n",
      "Epoch 34/100\n",
      "1021/1021 - 0s - loss: 0.4372 - val_loss: 0.4200\n",
      "Epoch 35/100\n",
      "1021/1021 - 0s - loss: 0.3938 - val_loss: 0.4196\n",
      "Epoch 36/100\n",
      "1021/1021 - 0s - loss: 0.4111 - val_loss: 0.4191\n",
      "Epoch 37/100\n",
      "1021/1021 - 0s - loss: 0.4102 - val_loss: 0.4140\n",
      "Epoch 38/100\n",
      "1021/1021 - 0s - loss: 0.3953 - val_loss: 0.4142\n",
      "Epoch 39/100\n",
      "1021/1021 - 0s - loss: 0.3971 - val_loss: 0.4131\n",
      "Epoch 40/100\n",
      "1021/1021 - 0s - loss: 0.4104 - val_loss: 0.4150\n",
      "Epoch 41/100\n",
      "1021/1021 - 0s - loss: 0.3837 - val_loss: 0.4156\n",
      "Epoch 42/100\n",
      "1021/1021 - 0s - loss: 0.3788 - val_loss: 0.4105\n",
      "Epoch 43/100\n",
      "1021/1021 - 0s - loss: 0.3779 - val_loss: 0.4127\n",
      "Epoch 44/100\n",
      "1021/1021 - 0s - loss: 0.3708 - val_loss: 0.4172\n",
      "Epoch 45/100\n",
      "1021/1021 - 0s - loss: 0.3403 - val_loss: 0.4168\n",
      "Epoch 46/100\n",
      "1021/1021 - 0s - loss: 0.3557 - val_loss: 0.4104\n",
      "Epoch 47/100\n",
      "1021/1021 - 0s - loss: 0.3445 - val_loss: 0.4118\n",
      "Epoch 48/100\n",
      "1021/1021 - 0s - loss: 0.3438 - val_loss: 0.4095\n",
      "Epoch 49/100\n",
      "1021/1021 - 0s - loss: 0.3458 - val_loss: 0.4077\n",
      "Epoch 50/100\n",
      "1021/1021 - 0s - loss: 0.3585 - val_loss: 0.4044\n",
      "Epoch 51/100\n",
      "1021/1021 - 0s - loss: 0.3382 - val_loss: 0.4086\n",
      "Epoch 52/100\n",
      "1021/1021 - 0s - loss: 0.3435 - val_loss: 0.4090\n",
      "Epoch 53/100\n",
      "1021/1021 - 0s - loss: 0.3490 - val_loss: 0.4100\n",
      "Epoch 54/100\n",
      "1021/1021 - 0s - loss: 0.3250 - val_loss: 0.4087\n",
      "Epoch 55/100\n",
      "1021/1021 - 0s - loss: 0.3263 - val_loss: 0.4104\n",
      "Epoch 56/100\n",
      "1021/1021 - 0s - loss: 0.2940 - val_loss: 0.4140\n",
      "Epoch 57/100\n",
      "1021/1021 - 0s - loss: 0.3123 - val_loss: 0.4122\n",
      "Epoch 58/100\n",
      "1021/1021 - 0s - loss: 0.3048 - val_loss: 0.4075\n",
      "Epoch 59/100\n",
      "1021/1021 - 0s - loss: 0.2958 - val_loss: 0.4092\n",
      "Epoch 60/100\n",
      "1021/1021 - 0s - loss: 0.3173 - val_loss: 0.4069\n",
      "Epoch 00060: early stopping\n",
      "1021/1021 [==============================] - 0s 92us/sample\n",
      "[[0.         0.        ]\n",
      " [0.80569106 0.79268293]\n",
      " [0.80243902 0.78943089]\n",
      " [0.80162602 0.78861789]\n",
      " [0.80243902 0.7902439 ]\n",
      " [0.80243902 0.78943089]\n",
      " [0.80162602 0.7902439 ]\n",
      " [0.80406504 0.78861789]\n",
      " [0.80487805 0.78943089]\n",
      " [0.80487805 0.7902439 ]\n",
      " [0.80406504 0.78861789]\n",
      " [0.80487805 0.78861789]\n",
      " [0.80569106 0.78861789]]\n"
     ]
    }
   ],
   "source": [
    "import sys, os, datetime, h5py\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense,  Dropout, Activation, Concatenate, Lambda\n",
    "#from tensorflow.python.keras.summary import merge\n",
    "from tensorflow.keras.utils import plot_model, multi_gpu_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from functools import partial, update_wrapper\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "np.random.seed(np.random.randint(100000))\n",
    "\n",
    "datadir = r\"C:\\Users\\onoda\\Documents\\Python\\DeepSurvival\"\n",
    "\n",
    "isAAL = 0\n",
    "combined = 0\n",
    "withAge = 0\n",
    "withMMSE = 0;\n",
    "\n",
    "CVs = 2\n",
    "Itr = 1 \n",
    "DROPOUT_RATIO = 0.5\n",
    "NB_EPOCH = 100\n",
    "BATCH_SIZE = 128\n",
    "N_GPUS = 1\n",
    "#MRI = 2 # 1:1.5T 2: 3.0T\n",
    "\n",
    "for ii in range(Itr):\n",
    "    \n",
    "    os.chdir( datadir )\n",
    "    \n",
    "    if isAAL == 0:\n",
    "        data = io.loadmat(\"GMV.mat\")\n",
    "        gmv = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        data = io.loadmat(\"GMV_Shimane.mat\")\n",
    "        gmv_shimaneH = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        data = io.loadmat(\"GMV_Dock.mat\")\n",
    "        gmv_dock = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "    else:\n",
    "        data = io.loadmat(\"GMV_AAL.mat\")\n",
    "        gmv = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        data = io.loadmat(\"GMV_Shimane_AAL.mat\")\n",
    "        gmv_shimaneH = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        data = io.loadmat(\"GMV_Dock_AAL.mat\")\n",
    "        gmv_dock = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        \n",
    "    if combined == 1:\n",
    "        data = io.loadmat(\"GMV.mat\")\n",
    "        gmv1 = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        data = io.loadmat(\"GMV_AAL.mat\")\n",
    "        gmv2 = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        gmv = np.concatenate([gmv1, gmv2], axis=1)\n",
    "        data = io.loadmat(\"GMV_Shimane.mat\")\n",
    "        gmv_shimaneH1 = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        data = io.loadmat(\"GMV_Dock.mat\")\n",
    "        gmv_dock1 = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        data = io.loadmat(\"GMV_Shimane_AAL.mat\")\n",
    "        gmv_shimaneH2 = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        data = io.loadmat(\"GMV_Dock_AAL.mat\")\n",
    "        gmv_dock2 = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "        gmv_shimaneH = np.concatenate([gmv_shimaneH1, gmv_shimaneH2], axis=1)\n",
    "        gmv_dock = np.concatenate([gmv_dock1, gmv_dock2], axis=1)\n",
    "\n",
    "\n",
    "    data  = io.loadmat(\"Demo_MMSE_corrected.mat\" )\n",
    "    demo = np.array( data[\"Demo\"], dtype = 'float32'  )   # database group mri subjectID age sex convert interval MMSE\n",
    "    data  = io.loadmat(\"Demo_Shimane.mat\" )\n",
    "    demo_shimaneH = np.array( data[\"Demo_Shimane\"], dtype = 'float32'  )   # age sex convert interval\n",
    "    data  = io.loadmat(\"Demo_Dock.mat\" )\n",
    "    demo_dock = np.array( data[\"Demo\"], dtype = 'float32'  )   # age sex convert interval mmse\n",
    "\n",
    "    index1 = (demo_shimaneH[:,2]==0)*(demo_shimaneH[:,0]>100)\n",
    "    index2 = (demo_shimaneH[:,0]<=63)\n",
    "    nn_shimaneH = np.sum(index1==0)\n",
    "    nn_dock = np.sum(index2==0)\n",
    "    gmv_shimane = np.concatenate([gmv_shimaneH[index1==0,:], gmv_dock],axis=0)\n",
    "    demo_shimane = np.concatenate([demo_shimaneH[index1==0,:], demo_dock],axis=0)\n",
    "    nn_shimane = demo_shimane.shape[0]\n",
    "    print(gmv_shimane.shape)\n",
    "    print(demo_shimane.shape)\n",
    "    features_test = gmv_shimane;\n",
    "    \n",
    "    if withAge == 1:\n",
    "        age = demo_shimane[:,0]/100\n",
    "        age = age[:,np.newaxis]\n",
    "        features_test = np.concatenate([features_test, age], axis=1)\n",
    "    if withMMSE == 1:\n",
    "        mmse = demo_shimane[:,4]/30\n",
    "        mmse = mmse[:,np.newaxis]\n",
    "        features_test = np.concatenate([features_test, mmse], axis=1)\n",
    "\n",
    "    if withMMSE == 1:\n",
    "        target = (demo[:,1] > 1)*(demo[:,7]>0) * (demo[:,8]>0)\n",
    "    else:\n",
    "        target = (demo[:,1] > 1)*(demo[:,7]>0)\n",
    "\n",
    "    target[gmv[:,0]==0] = 0\n",
    "    target_ADNI = (demo[target,0] == 1)\n",
    "    target_AIBL = (demo[target,0] == 2)\n",
    "    target_JADNI = (demo[target,0] == 3)\n",
    "\n",
    "    features = gmv[ target, :]\n",
    "    if withAge == 1:\n",
    "        age = demo[ target, 4]/100\n",
    "        age = age[:,np.newaxis]\n",
    "        features = np.concatenate([features, age], axis=1)\n",
    "    if withMMSE == 1:\n",
    "        mmse = demo[ target, 8]/30\n",
    "        mmse = mmse[:,np.newaxis]\n",
    "        features = np.concatenate([features, age], axis=1)\n",
    "    n_features =   features.shape[1]\n",
    "    e = np.array(demo[ target, 6], dtype = 'int32')\n",
    "    t = demo[ target, 7]\n",
    "\n",
    "    nn = features.shape[0]\n",
    "    training_sample = np.arange(nn)\n",
    "    np.random.shuffle(training_sample)\n",
    "    I = np.argsort(training_sample)\n",
    "    cv = training_sample % CVs\n",
    "    features = features[training_sample,:]\n",
    "    e = e[training_sample]\n",
    "    t = t[training_sample]\n",
    "    #J = np.int(np.ceil(np.max(t)) + 1)\n",
    "    J = np.int( np.round( np.max(t) ) + 1 )\n",
    "    print('time points:',J)\n",
    "    print('samples: ', nn)\n",
    "\n",
    "    #T = np.ceil(t).astype('int64')\n",
    "    T = np.round(t).astype('int64')\n",
    "    T[T==0] = 1\n",
    "    E = np.zeros([nn,J])\n",
    "    mask = np.ones([nn,J]).astype('float32')\n",
    "    for ii in range(nn):\n",
    "        if e[ii] == 1:\n",
    "            E[ii,T[ii]:J] = 1\n",
    "        if e[ii] == 0:\n",
    "            mask[ii,T[ii]+1:J] = 0\n",
    "\n",
    "    mask_shimane = np.ones([nn,J]).astype('float32')\n",
    "    nn_shimane = features_test.shape[0]\n",
    "    e_shimane = np.array(demo_shimane[ :, 2], dtype = 'int32')\n",
    "    E_shimane = np.zeros([nn_shimane,J])\n",
    "    T_shimane = np.round(demo_shimane[ :, 3]).astype('int64')\n",
    "    T_shimane[T_shimane==0] = 1\n",
    "    for ii in range(nn_shimane):\n",
    "        if e_shimane[ii] == 1:\n",
    "            E_shimane[ii,T_shimane[ii]:J] = 1\n",
    "        if e_shimane[ii] == 0:\n",
    "            mask_shimane[ii,T_shimane[ii]+1:J] = 0\n",
    "\n",
    "    todaydetail  =    datetime.datetime.today()\n",
    "    logdir = datadir + \"\\log_DeepSurv_Demo\"\n",
    "    os.makedirs(logdir, exist_ok = True)\n",
    "    os.chdir(logdir)\n",
    "    experiment_name = 'deepsurv'\n",
    "\n",
    "    from tensorflow.python.client import device_lib\n",
    "    device_lib.list_local_devices()\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "\n",
    "        def output_of_lambda(input_shape):\n",
    "            shape = list(input_shape)\n",
    "            return (shape[0], J)\n",
    "\n",
    "        def weibull_cdf(parameters):\n",
    "            m = parameters[:,0]\n",
    "            s = tf.maximum( parameters[:,1], 0.001 )\n",
    "            output_list = []\n",
    "            for ii in range( J ):\n",
    "                Time   = tf.constant( ii, dtype=\"float32\")\n",
    "                e_Time = tf.pow( Time, m )\n",
    "                s_Time = tf.negative( tf.div( e_Time, s) )\n",
    "                x = tf.subtract( tf.constant(1, dtype=\"float32\") , tf.exp( s_Time ) ) # F(t) = 1 - exp(-(t-g)^m/s) #ref http://www.mogami.com/notes/weibull.html\n",
    "                output_list.append ( x )\n",
    "            return tf.stack(output_list, axis=1)\n",
    "\n",
    "        def generator_loss(y_true, y_pred, weights):  # y_true's shape=(batch_size, row, col, ch)\n",
    "            #loss = tf.cumsum( tf.multiply( tf.square( tf.subtract( y_pred, y_true ) ), weights ), axis=1, reverse=True)[:,0]\n",
    "            log_p = tf.log( tf.add( y_pred,  tf.constant(1.0) ) )\n",
    "            log_t = tf.log( tf.add( y_true,  tf.constant(1.0) ) )\n",
    "            loss = tf.cumsum( tf.multiply( tf.square( tf.subtract( log_p, log_t ) ), weights ), axis=1, reverse=True)[:,0]\n",
    "            return loss\n",
    "\n",
    "        def wrapped_generator_loss(func, *args, **kwargs):\n",
    "            partial_generator_loss = partial(generator_loss, *args, **kwargs)\n",
    "            update_wrapper(partial_generator_loss, generator_loss)\n",
    "            return partial_generator_loss\n",
    "\n",
    "        inputs = Input((n_features,), name='inputs')\n",
    "        x1 = Dense(units=32, activation='relu', name='hidden_layer1')(inputs)\n",
    "        x1 = Dropout(DROPOUT_RATIO)(x1)\n",
    "        x2 = Dense(units=32, activation='relu', name='hidden_layer2')(x1)\n",
    "        x2 = Dropout(DROPOUT_RATIO)(x2)\n",
    "        x3 = Dense(units=32, activation='relu', name='hidden_layer3')(x2)\n",
    "        x3 = Dropout(DROPOUT_RATIO)(x3)\n",
    "        p1 = Dense(units=1, activation='softplus', name='param1_layer')(x3)\n",
    "        p2 = Dense(units=1, activation='relu', name='param2_layer')(x3)\n",
    "        parameters = Concatenate(name='params_layer')([p1, p2])\n",
    "        y_pred = Lambda(weibull_cdf, output_shape=output_of_lambda)(parameters)\n",
    "\n",
    "        mask_batch = Input((J,), name='mask_bartch')\n",
    "        L = wrapped_generator_loss(generator_loss, weights=mask_batch)\n",
    "\n",
    "        model = Model(inputs= [inputs, mask_batch], outputs = y_pred)\n",
    "        model.summary()\n",
    "\n",
    "    if  N_GPUS>=2:\n",
    "        models = multi_gpu_model(model, gpus=N_GPUS)\n",
    "    else:\n",
    "        models = model\n",
    "\n",
    "    pred_params = np.zeros([nn,2])\n",
    "    c_index_shimane = np.zeros([J,CVs])\n",
    "    for num in range(CVs):\n",
    "\n",
    "        x_train = features[cv != num,:]\n",
    "        y_train = E[cv != num,:]\n",
    "        mask_train = mask[cv != num,:]\n",
    "        x_test = features[cv == num,:]\n",
    "        y_test = E[cv == num,:]\n",
    "        mask_test = mask[cv == num,:]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        x_val = scaler.transform(features_test)\n",
    "\n",
    "        todaydetail  =    datetime.datetime.today()\n",
    "        outputfilename     = 'Training___CV' + str(num) + '_Demo.csv'\n",
    "        weightfilename     = 'WeightBest_CV' + str(num) + '_Demo.h5'\n",
    "\n",
    "        checkpointer = ModelCheckpoint(filepath=weightfilename, monitor='loss', verbose=1, save_best_only=True)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',patience=10,verbose=1)\n",
    "        callbacks = []\n",
    "        callbacks.append(early_stopping)\n",
    "        callbacks.append(CSVLogger(outputfilename))\n",
    "        #callbacks.append(checkpointer)\n",
    "\n",
    "        adm = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "        models.compile(optimizer=adm, loss=L)\n",
    "        #models.compile(optimizer='Adam', loss='mean_squared_error', metrics=[\"accuracy\"])\n",
    "        session = K.get_session()\n",
    "        if num==0:\n",
    "            for layer in model.layers:\n",
    "                if hasattr(layer, 'kernel_initializer'):\n",
    "                    layer.kernel.initializer.run(session=session)\n",
    "                    model.save_weights('InitialWeights.h5')\n",
    "                    print('save initial weights')\n",
    "        elif num>0:\n",
    "            model.load_weights('InitialWeights.h5')\n",
    "            print('load inital weights')\n",
    "\n",
    "        models.fit([x_train, mask_train], y_train, batch_size=BATCH_SIZE, epochs = NB_EPOCH, callbacks=callbacks, verbose=2, validation_data = ([x_test, mask_test], y_test))\n",
    "        model.save_weights('Model_Weights_' + str(num+1) + '.h5')\n",
    "        \n",
    "        \n",
    "        #intermediate_model = Model(inputs=model.input, outputs=model.get_layer('params_layer').output)\n",
    "\n",
    "        #models.load_weights(weightfilename)\n",
    "        #models.compile(optimizer='Adam', loss=L)\n",
    "        prob = models.predict([x_test, mask_test], batch_size=BATCH_SIZE, verbose=1)\n",
    "        intermediate_model = Model(inputs=model.input, outputs=model.get_layer('params_layer').output)\n",
    "        intermediate_output = intermediate_model.predict([x_test, mask_test])\n",
    "        pred_params[cv == num,:] = intermediate_output\n",
    "\n",
    "        pred_params_shimane = np.zeros([nn_shimane,2])\n",
    "        pred_params_shimane = intermediate_model.predict([x_val, mask_shimane])\n",
    "        pred_prob_shimane = np.zeros([nn_shimane,J])\n",
    "        for tt in range(J):\n",
    "            pred_prob_shimane[:,tt] = 1 - np.exp( - tt ** pred_params_shimane[:,0]  / pred_params_shimane[:,1] );\n",
    "        todaydetail  =    datetime.datetime.today()\n",
    "        predictionfilename = 'Para_' + str(num+1) + '_Demo.csv'\n",
    "        prediction = np.c_[pred_params_shimane, pred_prob_shimane]\n",
    "        np.savetxt(predictionfilename, prediction, delimiter=',')\n",
    "\n",
    "        for num2 in range(J - 1) :\n",
    "            c_index_shimane[num2+1,num] = concordance_index(T_shimane,1/pred_prob_shimane[:,num2+1], e_shimane)\n",
    "        print( c_index_shimane )\n",
    "\n",
    "    pred_prob = np.zeros([nn,J])\n",
    "    for tt in range(J):\n",
    "        pred_prob[:,tt] = 1 - np.exp( - tt ** pred_params[:,0]  / pred_params[:,1] );\n",
    "\n",
    "    todaydetail  =    datetime.datetime.today()\n",
    "    predictionfilename = 'Prediction_CV' + str(CVs) + '_Demo.csv'\n",
    "    prediction = np.c_[training_sample[I], demo[target,0:6], e[I], t[I], pred_params[I,:], pred_prob[I,:], I]\n",
    "    np.savetxt(predictionfilename, prediction, delimiter=',')\n",
    "\n",
    "    c_index = np.zeros([J])\n",
    "    for num in range(J - 1) :\n",
    "        c_index[num+1] = concordance_index(T,1/pred_prob[:,num+1], e)\n",
    "\n",
    "    c_index_ADNI = np.zeros([J])\n",
    "    for num in range(J - 1) :\n",
    "        c_index_ADNI[num+1] = concordance_index(T[target_ADNI],1/pred_prob[target_ADNI,num+1], e[target_ADNI])\n",
    "\n",
    "    c_index_AIBL = np.zeros([J])\n",
    "    for num in range(J - 1) :\n",
    "        c_index_AIBL[num+1] = concordance_index(T[target_AIBL],1/pred_prob[target_AIBL,num+1], e[target_AIBL])\n",
    "\n",
    "    c_index_JADNI = np.zeros([J])\n",
    "    for num in range(J - 1) :\n",
    "        c_index_JADNI[num+1] = concordance_index(T[target_JADNI],1/pred_prob[target_JADNI,num+1], e[target_JADNI])\n",
    "\n",
    "    cindexfilename = 'C_index_' + 'Demo.txt'\n",
    "    np.savetxt(cindexfilename, (c_index,c_index_ADNI, c_index_AIBL, c_index_JADNI))\n",
    "    cindexfilename2 = 'C_index_shiamane_' + 'Demo.txt'\n",
    "    np.savetxt(cindexfilename2, c_index_shimane)\n",
    "\n",
    "    #json_string = models.to_json()\n",
    "    #modeltxtfilename   = 'Modeltxt_' + 'Demo.txt'\n",
    "    #f = open(modeltxtfilename,'w')\n",
    "    #f.write(json_string)\n",
    "    #f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\onoda\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\onoda\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\onoda\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\onoda\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\onoda\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             (None, 246)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer1 (Dense)           (None, 32)           7904        inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32)           0           hidden_layer1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer2 (Dense)           (None, 32)           1056        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           hidden_layer2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer3 (Dense)           (None, 32)           1056        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           hidden_layer3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "param1_layer (Dense)            (None, 1)            33          dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "param2_layer (Dense)            (None, 1)            33          dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "params_layer (Concatenate)      (None, 2)            0           param1_layer[0][0]               \n",
      "                                                                 param2_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 12)           0           params_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,082\n",
      "Trainable params: 10,082\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from functools import partial, update_wrapper\n",
    "from keras.layers import Input,InputLayer, Dense,  Dropout, Activation, Concatenate, Lambda \n",
    "from keras.models import Model, model_from_json\n",
    "\n",
    "J = 12\n",
    "\n",
    "def output_of_lambda(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    return (shape[0], J)\n",
    "\n",
    "def weibull_cdf(parameters):\n",
    "    m = parameters[:,0]\n",
    "    s = tf.maximum( parameters[:,1], 0.001 )\n",
    "    output_list = []\n",
    "    for ii in range( J ):\n",
    "        Time   = tf.constant( ii, dtype=\"float32\")\n",
    "        e_Time = tf.pow( Time, m )\n",
    "        s_Time = tf.negative( tf.div( e_Time, s) )\n",
    "        x = tf.subtract( tf.constant(1, dtype=\"float32\") , tf.exp( s_Time ) ) # F(t) = 1 - exp(-(t-g)^m/s) #ref http://www.mogami.com/notes/weibull.html\n",
    "        output_list.append ( x )\n",
    "    return tf.stack(output_list, axis=1)\n",
    "\n",
    "def generator_loss(y_true, y_pred, weights):  # y_true's shape=(batch_size, row, col, ch)\n",
    "    #loss = tf.cumsum( tf.multiply( tf.square( tf.subtract( y_pred, y_true ) ), weights ), axis=1, reverse=True)[:,0]\n",
    "    log_p = tf.log( tf.add( y_pred,  tf.constant(1.0) ) )\n",
    "    log_t = tf.log( tf.add( y_true,  tf.constant(1.0) ) )\n",
    "    loss = tf.cumsum( tf.multiply( tf.square( tf.subtract( log_p, log_t ) ), weights ), axis=1, reverse=True)[:,0]\n",
    "    return loss\n",
    "\n",
    "def wrapped_generator_loss(func, *args, **kwargs):\n",
    "    partial_generator_loss = partial(generator_loss, *args, **kwargs)\n",
    "    update_wrapper(partial_generator_loss, generator_loss)\n",
    "    return partial_generator_loss\n",
    "\n",
    "inputs = Input((246,), name='inputs')\n",
    "x1 = Dense(units=32, activation='relu', name='hidden_layer1')(inputs)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "x2 = Dense(units=32, activation='relu', name='hidden_layer2')(x1)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "x3 = Dense(units=32, activation='relu', name='hidden_layer3')(x2)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "p1 = Dense(units=1, activation='softplus', name='param1_layer')(x3)\n",
    "p2 = Dense(units=1, activation='relu', name='param2_layer')(x3)\n",
    "parameters = Concatenate(name='params_layer')([p1, p2])\n",
    "y_pred = Lambda(weibull_cdf, output_shape=output_of_lambda)(parameters)\n",
    "\n",
    "mask_batch = Input((J,), name='mask_bartch')\n",
    "L = wrapped_generator_loss(generator_loss, weights=mask_batch)\n",
    "\n",
    "model = Model(inputs= [inputs, mask_batch], outputs = y_pred)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InitialWeights.h5', 'Model_Weights_1.h5', 'Model_Weights_2.h5']\n"
     ]
    }
   ],
   "source": [
    "import os, scipy, glob, innvestigate\n",
    "os.chdir( datadir + \"\\log_DeepSurv_Demo\") \n",
    "h5files = glob.glob('*.h5')\n",
    "print(h5files)\n",
    "\n",
    "for ii in range(CVs):\n",
    "    model.load_weights(h5files[ii])\n",
    "    intermediate_model = Model(inputs=model.get_layer('inputs').input, outputs=model.get_layer('param1_layer').output)\n",
    "\n",
    "    analyzer = innvestigate.create_analyzer(\"deep_taylor\", intermediate_model)\n",
    "    data = analyzer.analyze(gmv)\n",
    "    dt_filename = 'deep_taylor_' + str(ii+1)  + '.mat'\n",
    "    scipy.io.savemat(dt_filename, {'deep_taylor':data})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
